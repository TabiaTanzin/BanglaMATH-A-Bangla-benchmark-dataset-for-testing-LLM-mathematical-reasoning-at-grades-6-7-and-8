# BanglaMATH: A Dataset for Evaluating Mathematical Reasoning in Bangla

## Overview

BanglaMATH is a dataset designed to evaluate the mathematical reasoning capabilities of Large Language Models (LLMs) in Bangla. With the growing use of LLMs in education and AI research, there is a critical gap in their performance for low-resource languages like Bangla. The dataset consists of **1.7k Bangla math word problems** sourced from elementary school workbooks and spans topics such as **Arithmetic**, **Algebra**, **Geometry**, and **Logical Reasoning**.

In our study, we evaluate both commercial and open-source LLMs using this dataset. The dataset is annotated with additional details, including **grade level** and the **number of reasoning steps** required to solve each problem.

We specifically designed BanglaMATH to help assess the mathematical reasoning capabilities of LLMs in Bangla and to highlight the current limitations of LLMs in this domain.

This work has been **accepted at the EMNLP MathNLP Workshop 2025**.

![Sample Problem](Workflow_BMATH.drawio (1).png)

_Figure 1: An example from the BanglaMATH dataset illustrating a discrepancy between the correct human-provided
explanation and the incorrect response generated by ChatGPT-4o (as of May 18, 2025).._


## Dataset Details

- **Number of problems**: 1,700+ Bangla math word problems
- **Topics covered**:
  - Arithmetic
  - Algebra
  - Geometry
  - Logical Reasoning
- **Annotations**:
  - Grade level (e.g., Grade 6, 7, 8)
  - Number of reasoning steps required
- **Source**: Bangla elementary school workbooks

## LLM Evaluation

We evaluate multiple LLMs on this dataset, and the results highlight the performance of different models:

- **Top-performing models**:
  - **Gemini 2.5 Flash**
  - **DeepSeek V3**

These models achieved an accuracy of **â‰¥ 80%** across three elementary school grades.

### Performance Insights:
- **Zero-shot evaluation approach**: No auxiliary prompting strategies were used. The models are presented with math problems in their original Bangla text format, without additional context, examples, or instructions.
- **Performance Bias**: Both models showed significant performance bias when translated into English and struggled with robustness when distracted with irrelevant information.
- **Language Bias**: Despite strong performance in English, the top models exhibited a notable performance drop in Bangla.

## Research Contributions

This study reveals the following:

- **Limitations in LLMs for low-resource languages**: LLMs face challenges in handling mathematical reasoning in languages like Bangla.
- **Need for further research**: The study underscores the need for further development in multilingual and equitable mathematical understanding, especially for low-resource languages.
- **BanglaMATH** provides a benchmark for future research on LLMs in low-resource languages.

## Authors

- **Tabia Tanzin Prama**  
- **Christopher M. Danforth**  
- **Peter Sheridan Dodds**



## License

This dataset is made available for research purposes only. Please refer to the license terms provided in the dataset package.

## Contact

For more information, please contact the authors
